import os
import pandas as pd
import numpy as np
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
import spacy
from spacy.lang.en import English
import pyLDAvis
import pyLDAvis.gensim
from gensim.models import CoherenceModel
import nltk
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")

# converting sentences to list of words


def sent_to_words(sentences):
    for sentence in sentences:
        # convert sentence (string) to list of words ignoring any words
        # that are too small (len 2) or too large (len 15)
        # print(sentence)
        yield (gensim.utils.simple_preprocess(str(sentence)))

# lemmatization keeping only noun, adjective, verb and adverb


def lemmatization(texts, allowed_postags=["ADJ", "ADV"]):
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append(
            [token.lemma_ for token in doc if token.pos_ in allowed_postags]
        )
    return texts_out


# download stopwords from nltk
nltk.download('stopwords')

# reading data
start_time = datetime.now()
data = pd.read_csv('rss_feed_1.csv')
data1 = data['article_title'].unique()
data = pd.DataFrame(data1, columns=['article_title'])
stop_words = stopwords.words("english")
stop_words.extend(['would', 'could', 'dont', 'wont',
                   'should', "\ufeff1", 'get', 'too', 'to', 'with'])
data1 = data.values.tolist()
print(
    "Time taken to read input: %s" % str(
        datetime.now() - start_time
    )[:-3]
)
# tokenzing sentences to list of words,removing punctuations and unnecessary characters
start_time = datetime.now()
data1_words = list(sent_to_words(data1))
print(data1_words[:1])
print(
    "Time taken to for simple_preprocess: %s" % str(
        datetime.now() - start_time
    )[:-3]
)

# removing stopwords for list of words
start_time = datetime.now()
data1_words_nostops = (
    [word for word in simple_preprocess(str(doc)) if word not in stop_words]
    for doc in data1_words)
print(
    "Time taken to remove stopwords: %s" % str(
        datetime.now() - start_time
    )[:-3]
)
# forming bigrams and trigrams
# if threshold value is high then few phrases available
start_time = datetime.now()
bigram = gensim.models.Phrases(data1_words, min_count=5, threshold=100)
trigram = gensim.models.Phrases(bigram[data1_words], threshold=100)
bigram_model = gensim.models.phrases.Phraser(bigram)
trigram_model = gensim.models.phrases.Phraser(trigram)

data1_words_bigrams = [bigram_model[doc] for doc in data1_words_nostops]
trigram_words = [trigram_model[bigram_model[doc]]
                 for doc in data1_words_nostops]
print(
    "Time taken for bigram and trigram models: %s" % str(
        datetime.now() - start_time
    )[:-3]
)

print(len(trigram_words))

# loading spacy 'english i.e en module'
nlp = spacy.load("en", disable=["parser", "ner"])

# lemmatizing keeping only noun,adjective,verb and adverb
start_time = datetime.now()
data1_lemmatized = lemmatization(
    data1_words_bigrams, allowed_postags=["ADJ", "ADV"]
)
print(data1_lemmatized[:1])
print(
    "Time taken for lemmatization: %s" % str(
        datetime.now() - start_time
    )[:-3]
)

# creating dictionary and corpus for lda_model
start_time = datetime.now()
id2word = corpora.Dictionary(data1_lemmatized)
corpus = [id2word.doc2bow(text) for text in data1_lemmatized]
print(
    "Time taken for dictionary and corpus: %s" % str(
        datetime.now() - start_time
    )[:-3]
)

# LDA model

start_time = datetime.now()
lda_model = gensim.models.ldamodel.LdaModel(
    corpus=corpus,
    id2word=id2word,
    num_topics=10, random_state=10,
    update_every=1,
    chunksize=100,
    passes=10,
    alpha='auto',
    per_word_topics=True
)
print(
    "Time taken for lda_model: %s" % str(
        datetime.now() - start_time
    )[:-3]
)
print(lda_model.print_topics())

# visulazing topics with keywords using pyLDAvis
vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
pyLDAvis.save_html(vis, "output/LDA_Visualization_rss_feed_10.html")
preplexity = lda_model.log_perplexity(corpus)
print(preplexity)
coherence_model = CoherenceModel(
    model=lda_model, texts=data1_lemmatized, dictionary=id2word, coherence='c_v')
coherence = coherence_model.get_coherence()
print(coherence)


# def coherence_values(dictionary, corpus, texts, limit, start=1, step=2):
#     coherences = []
#     models_list = []
#     for num_topics in range(start, limit, step):
#         model = gensim.models.ldamodel.LdaModel(
#             corpus=corpus, id2word=id2word, num_topics=num_topics)
#         models_list.append(model)
#         coherence_model_lda = CoherenceModel(
#             model=model, texts=texts, dictionary=dictionary, coherence='c_v')
#         coherences.append(coherence_model_lda.get_coherence())

#     return models_list, coherences


# models_list, coherences = coherence_values(
#     dictionary=id2word, corpus=corpus, texts=data1_lemmatized, start=1, limit=100, step=2)
# limit = 100
# start = 1
# step = 2
# x = range(start, limit, step)
# plt.plot(x, coherences)
# plt.xlabel("Num Topics")
# plt.ylabel("Coherence score")
# plt.legend(("coherence_values"), loc='best')
# print(plt.show())
# for m, cv in zip(x, coherences):
#     print("Num Topics =", m, " has Coherence Value of", round(cv, 4))
